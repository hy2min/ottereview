import os
from dotenv import load_dotenv
from langchain.chat_models import init_chat_model
from typing import List, Dict, Any
import logging
from collections import defaultdict, Counter
import re
from .vector_db import vector_db
from models import PreparationResult, PRData  # PRDataëŠ” í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•´ ìœ ì§€
from .config_based_priority import calculate_config_based_priority_candidates

# .env íŒŒì¼ì—ì„œ í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

logger = logging.getLogger(__name__)

# OpenAI API KEY í™•ì¸
openai_api_key = os.getenv("OPENAI_API_KEY")
if not openai_api_key:
    raise ValueError("OPENAI_API_KEYê°€ í™˜ê²½ë³€ìˆ˜ì— ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")

# ì¤‘ìš”! langchainì˜ BASE_URLì„ GMSë¡œ ì„¤ì •í•©ë‹ˆë‹¤.
os.environ["OPENAI_API_BASE"] = "https://gms.ssafy.io/gmsapi/api.openai.com/v1"

model = init_chat_model("gpt-4o-mini", model_provider="openai")


def _extract_code_context_from_files(files: List[Any], max_files: int = 10) -> str:
    """
    íŒŒì¼ë“¤ì—ì„œ ì½”ë“œ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ì—¬ LLM ë¶„ì„ìš© ë¬¸ìì—´ ìƒì„±
    
    Args:
        files: FileChangeInfo ê°ì²´ë“¤ì˜ ë¦¬ìŠ¤íŠ¸
        max_files: ë¶„ì„í•  ìµœëŒ€ íŒŒì¼ ìˆ˜
        
    Returns:
        str: íŒŒì¼ë³„ ë³€ê²½ì‚¬í•­ ìš”ì•½ ì»¨í…ìŠ¤íŠ¸
    """
    context_parts = []
    
    for i, file in enumerate(files[:max_files]):
        file_info = []
        file_info.append(f"íŒŒì¼: {file.filename}")
        file_info.append(f"ìƒíƒœ: {file.status}")
        file_info.append(f"ë³€ê²½: +{file.additions}/-{file.deletions}")
        
        # íŒŒì¼ í™•ì¥ìë¡œ ì–¸ì–´ ì¶”ì •
        file_ext = file.filename.split('.')[-1].lower() if '.' in file.filename else 'unknown'
        file_info.append(f"íƒ€ì…: {file_ext}")
        
        # patch ë‚´ìš©ì´ ìˆìœ¼ë©´ ì£¼ìš” ë³€ê²½ì‚¬í•­ ì¶”ì¶œ
        if file.patch:
            key_changes = _extract_key_changes_from_patch(file.patch, file.filename)
            if key_changes:
                file_info.append(f"ì£¼ìš”ë³€ê²½: {key_changes}")
        
        context_parts.append(" | ".join(file_info))
    
    return "\n".join(context_parts)


def _extract_key_changes_from_patch(patch: str, filename: str) -> str:
    """
    patchì—ì„œ í•µì‹¬ ë³€ê²½ì‚¬í•­ì„ ì¶”ì¶œ
    
    Args:
        patch: git diff patch ë‚´ìš©
        filename: íŒŒì¼ëª…
        
    Returns:
        str: í•µì‹¬ ë³€ê²½ì‚¬í•­ ìš”ì•½
    """
    if not patch:
        return ""
    
    key_patterns = {
        'security': [r'auth', r'token', r'password', r'jwt', r'security', r'permission', r'role'],
        'database': [r'entity', r'repository', r'@Table', r'@Entity', r'@Column', r'migration', r'schema'],
        'api': [r'@RestController', r'@RequestMapping', r'@GetMapping', r'@PostMapping', r'@Controller', r'endpoint'],
        'config': [r'@Configuration', r'application\.', r'config', r'properties'],
        'business_logic': [r'service', r'business', r'logic', r'calculate', r'process', r'validate'],
        'test': [r'@Test', r'test', r'spec', r'mock', r'assert']
    }
    
    changes = []
    lines = patch.split('\n')
    
    # ì¶”ê°€/ì‚­ì œëœ ë¼ì¸ì—ì„œ íŒ¨í„´ ì°¾ê¸°
    for line in lines:
        if line.startswith('+') or line.startswith('-'):
            line_content = line[1:].strip().lower()
            
            for category, patterns in key_patterns.items():
                for pattern in patterns:
                    if re.search(pattern, line_content):
                        changes.append(f"{category}ê´€ë ¨")
                        break
    
    # ì¤‘ë³µ ì œê±° ë° ìƒìœ„ 3ê°œë§Œ
    unique_changes = list(set(changes))[:3]
    return ", ".join(unique_changes) if unique_changes else "ì¼ë°˜ì ì¸ ì½”ë“œ ë³€ê²½"


def _analyze_files_for_priority_context(files: List[Any]) -> Dict[str, Any]:
    """
    íŒŒì¼ë“¤ì„ ë¶„ì„í•˜ì—¬ ìš°ì„ ìˆœìœ„ ê²°ì •ì— í•„ìš”í•œ ì»¨í…ìŠ¤íŠ¸ ìƒì„±
    
    Args:
        files: FileChangeInfo ê°ì²´ë“¤ì˜ ë¦¬ìŠ¤íŠ¸
        
    Returns:
        Dict[str, Any]: íŒŒì¼ ë¶„ì„ ê²°ê³¼ ì»¨í…ìŠ¤íŠ¸
    """
    analysis = {
        'high_risk_files': [],
        'medium_risk_files': [],
        'low_risk_files': [],
        'file_categories': defaultdict(list),
        'total_changes': 0,
        'security_related': [],
        'database_related': [],
        'api_related': []
    }
    
    # ê³ ìœ„í—˜ íŒŒì¼ íŒ¨í„´
    high_risk_patterns = [
        r'auth', r'security', r'token', r'jwt', r'password', r'permission',
        r'config', r'properties', r'yml', r'yaml'
    ]
    
    # ì¤‘ìœ„í—˜ íŒŒì¼ íŒ¨í„´
    medium_risk_patterns = [
        r'controller', r'service', r'repository', r'entity', r'dto',
        r'api', r'endpoint', r'business'
    ]
    
    for file in files:
        filename_lower = file.filename.lower()
        file_changes = file.additions + file.deletions
        analysis['total_changes'] += file_changes
        
        # ìœ„í—˜ë„ ë¶„ë¥˜
        if any(re.search(pattern, filename_lower) for pattern in high_risk_patterns):
            analysis['high_risk_files'].append({
                'filename': file.filename,
                'changes': file_changes,
                'status': file.status
            })
        elif any(re.search(pattern, filename_lower) for pattern in medium_risk_patterns):
            analysis['medium_risk_files'].append({
                'filename': file.filename,
                'changes': file_changes,
                'status': file.status
            })
        else:
            analysis['low_risk_files'].append({
                'filename': file.filename,
                'changes': file_changes,
                'status': file.status
            })
        
        # ì¹´í…Œê³ ë¦¬ë³„ ë¶„ë¥˜
        if re.search(r'auth|security|token|jwt|password', filename_lower):
            analysis['security_related'].append(file.filename)
        if re.search(r'entity|repository|migration|schema', filename_lower):
            analysis['database_related'].append(file.filename)
        if re.search(r'controller|api|endpoint|rest', filename_lower):
            analysis['api_related'].append(file.filename)
    
    return analysis


def _select_relevant_files_for_candidate(files_analysis: Dict[str, Any], candidate_type: str) -> List[str]:
    """
    í›„ë³´ íƒ€ì…ì— ë”°ë¼ ê´€ë ¨ëœ íŒŒì¼ë“¤ì„ ì„ íƒ
    
    Args:
        files_analysis: íŒŒì¼ ë¶„ì„ ê²°ê³¼
        candidate_type: í›„ë³´ ìœ í˜• (security, database, api, business_logic, etc.)
        
    Returns:
        List[str]: ê´€ë ¨ íŒŒì¼ íŒ¨ìŠ¤ë“¤
    """
    if candidate_type == 'security':
        relevant_files = files_analysis['security_related']
        relevant_files.extend([f['filename'] for f in files_analysis['high_risk_files'][:3]])
    elif candidate_type == 'database':
        relevant_files = files_analysis['database_related']
    elif candidate_type == 'api':
        relevant_files = files_analysis['api_related']
    else:
        # ì¼ë°˜ì ì¸ ê²½ìš°: ë³€ê²½ì‚¬í•­ì´ í° íŒŒì¼ë“¤
        all_files = (files_analysis['high_risk_files'] + 
                     files_analysis['medium_risk_files'] + 
                     files_analysis['low_risk_files'])
        all_files.sort(key=lambda x: x['changes'], reverse=True)
        relevant_files = [f['filename'] for f in all_files[:5]]
    
    # ì¤‘ë³µ ì œê±° ë° ìµœëŒ€ 5ê°œë¡œ ì œí•œ
    return list(set(relevant_files))[:5]


async def recommend_priority(pr_data: PreparationResult) -> Dict[str, Any]:
    """
    RAGë¥¼ í™œìš©í•œ ë¦¬ë·° ìš°ì„ ìˆœìœ„ ì¶”ì²œ ë¡œì§ - 3ê°œì˜ í›„ë³´ ë°˜í™˜ (íŒŒì¼ íŒ¨ìŠ¤ í¬í•¨)
    
    Args:
        pr_data: PRData ê°ì²´ (í˜„ì¬ PR ì •ë³´)
        
    Returns:
        Dict[str, Any]: 3ê°œì˜ ìš°ì„ ìˆœìœ„ ì¶”ì²œ ê²°ê³¼ (ê´€ë ¨ íŒŒì¼ íŒ¨ìŠ¤ í¬í•¨)
    """
    try:
        # ğŸš¨ ìˆ˜ì •ëœ ë¶€ë¶„: ë¶„ì„ì—ì„œ ì œì™¸í•  íŒŒì¼ í™•ì¥ì ì •ì˜
        IGNORE_EXTENSIONS = ['.txt', '.md', '.log', '.gitignore', '.lock', '.properties', '.yml', '.yaml']
        
        # ğŸš¨ ìˆ˜ì •ëœ ë¶€ë¶„: ìœ ì˜ë¯¸í•œ ì½”ë“œ íŒŒì¼ë§Œ í•„í„°ë§
        valid_files = [f for f in pr_data.files if not f.filename.endswith(tuple(IGNORE_EXTENSIONS))]

        if not valid_files:
            return _get_default_error_response_with_files(pr_data.files)

        # 1. íŒŒì¼ ë¶„ì„ì„ í†µí•œ ì»¨í…ìŠ¤íŠ¸ ìƒì„± (í•„í„°ë§ëœ íŒŒì¼ ì‚¬ìš©)
        files_analysis = _analyze_files_for_priority_context(valid_files)
        code_context = _extract_code_context_from_files(valid_files)
        
        # 2. ë²¡í„° DBì—ì„œ ìœ ì‚¬í•œ ìš°ì„ ìˆœìœ„ íŒ¨í„´ ê²€ìƒ‰ (RAGì˜ Retrieval ë‹¨ê³„)
        similar_patterns = await vector_db.get_similar_priority_patterns(pr_data, limit=15)
        
        if not similar_patterns:
            logger.warning("ìœ ì‚¬í•œ ìš°ì„ ìˆœìœ„ íŒ¨í„´ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. íŒŒì¼ ê¸°ë°˜ ë¶„ì„ìœ¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
            similar_patterns = " "
        
        # 3. ê²€ìƒ‰ëœ íŒ¨í„´ë“¤ì„ LLMìš© ì»¨í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
        context = _build_priority_context_from_patterns(similar_patterns)
        
        # 4. LLMì—ê²Œ ì»¨í…ìŠ¤íŠ¸ì™€ í•¨ê»˜ 3ê°œì˜ ìš°ì„ ìˆœìœ„ í›„ë³´ ë¶„ì„ ìš”ì²­ (RAGì˜ Generation ë‹¨ê³„)
        priority_candidates = await _generate_priority_candidates_with_llm(
            context, pr_data, code_context, files_analysis, valid_files
        )
        
        # 5. ê²°ê³¼ ê²€ì¦ ë° ë³´ì™„
        return _validate_and_complete_candidates(priority_candidates, pr_data, files_analysis)
        
    except Exception as e:
        logger.error(f"ìš°ì„ ìˆœìœ„ ì¶”ì²œ ì „ì²´ ê³¼ì •ì—ì„œ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        # ğŸš¨ ìˆ˜ì •ëœ ë¶€ë¶„: ì˜¤ë¥˜ ë°œìƒ ì‹œ í•„í„°ë§ëœ íŒŒì¼ë¡œ ê¸°ë³¸ ì‘ë‹µ ìƒì„±
        return _get_default_error_response_with_files(valid_files if 'valid_files' in locals() else pr_data.files)


def _get_default_error_response_with_files(files: List[Any]) -> Dict[str, Any]:
    """ê¸°ë³¸ ì˜¤ë¥˜ ì‘ë‹µ ìƒì„± (íŒŒì¼ ì •ë³´ í¬í•¨)"""
    # ë³€ê²½ì‚¬í•­ì´ í° íŒŒì¼ 3ê°œ ì„ íƒ
    sorted_files = sorted(files, key=lambda f: f.additions + f.deletions, reverse=True)[:3]
    common_files = [f.filename for f in sorted_files] if sorted_files else []
    
    return {
        'priority': [
            {
                'title': 'ìš°ì„ ìˆœìœ„ ì¶”ì²œì˜¤ë¥˜',
                'priority_level': 'MEDIUM',
                'reason': 'AI ì„œë¹„ìŠ¤ì—ì„œ ì˜¤ë¥˜ê°€ ë°œìƒí•˜ì˜€ê±°ë‚˜, íŒŒì¼ ë³€ê²½ì´ ì ìŠµë‹ˆë‹¤.',
                'related_files': common_files
            },
            {
                'title': 'ìš°ì„ ìˆœìœ„ ì¶”ì²œì˜¤ë¥˜',
                'priority_level': 'MEDIUM',
                'reason': 'AI ì„œë¹„ìŠ¤ì—ì„œ ì˜¤ë¥˜ê°€ ë°œìƒí•˜ì˜€ê±°ë‚˜, íŒŒì¼ ë³€ê²½ì´ ì ìŠµë‹ˆë‹¤.',
                'related_files': common_files
            },
            {
                'title': 'ìš°ì„ ìˆœìœ„ ì¶”ì²œì˜¤ë¥˜',
                'priority_level': 'MEDIUM',
                'reason': 'AI ì„œë¹„ìŠ¤ì—ì„œ ì˜¤ë¥˜ê°€ ë°œìƒí•˜ì˜€ê±°ë‚˜, íŒŒì¼ ë³€ê²½ì´ ì ìŠµë‹ˆë‹¤.',
                'related_files': common_files
            }
        ]
    }


def _validate_and_complete_candidates(result: Dict[str, Any], pr_data: PreparationResult, files_analysis: Dict[str, Any]) -> Dict[str, Any]:
    """ê²°ê³¼ë¥¼ ê²€ì¦í•˜ê³ , 3ê°œê°€ ì•ˆë˜ë©´ ì„¤ì • ê¸°ë°˜ ë˜ëŠ” ê¸°ë³¸ê°’ìœ¼ë¡œ ì±„ì›€ (íŒŒì¼ ì •ë³´ í¬í•¨)"""
    
    candidates = result.get('priority', [])
    valid_candidates = []

    for i, candidate in enumerate(candidates[:3]):
        if not isinstance(candidate, dict) or not all(k in candidate for k in ['title', 'priority_level', 'reason']):
            continue
        
        priority_level = candidate.get('priority_level', 'MEDIUM')
        if priority_level not in ['CRITICAL', 'HIGH', 'MEDIUM', 'LOW']:
            priority_level = 'MEDIUM'
        
        # related_filesê°€ ì—†ìœ¼ë©´ íŒŒì¼ ë¶„ì„ ê²°ê³¼ë¡œ ì¶”ê°€
        related_files = candidate.get('related_files', [])
        if not related_files:
            # í›„ë³´ì˜ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ê´€ë ¨ íŒŒì¼ ì¶”ì •
            candidate_content = (candidate.get('title', '') + ' ' + candidate.get('reason', '')).lower()
            if 'security' in candidate_content or 'auth' in candidate_content:
                related_files = _select_relevant_files_for_candidate(files_analysis, 'security')
            elif 'database' in candidate_content or 'entity' in candidate_content:
                related_files = _select_relevant_files_for_candidate(files_analysis, 'database')
            elif 'api' in candidate_content or 'controller' in candidate_content:
                related_files = _select_relevant_files_for_candidate(files_analysis, 'api')
            else:
                related_files = _select_relevant_files_for_candidate(files_analysis, 'general')
        
        valid_candidates.append({
            'title': candidate.get('title', f'ìš°ì„ ìˆœìœ„ í›„ë³´ {i+1}'),
            'priority_level': priority_level,
            'reason': candidate.get('reason', 'ë¶„ì„ ê²°ê³¼ì— ë”°ë¥¸ ìš°ì„ ìˆœìœ„'),
            'related_files': related_files[:5]  # ìµœëŒ€ 5ê°œë¡œ ì œí•œ
        })

    # 3ê°œ ë¯¸ë§Œì¸ ê²½ìš°, ë¨¼ì € ì„¤ì • ê¸°ë°˜ í›„ë³´ë¡œ ì±„ìš°ê³ , ê·¸ë˜ë„ ë¶€ì¡±í•˜ë©´ ê¸°ë³¸ ì˜¤ë¥˜ê°’ìœ¼ë¡œ ì±„ì›€
    if len(valid_candidates) < 3:
        try:
            config_candidates = calculate_config_based_priority_candidates(pr_data)['candidates']
            
            # ì„¤ì • ê¸°ë°˜ í›„ë³´ì—ì„œ ì¤‘ë³µë˜ì§€ ì•ŠëŠ” ê²ƒì„ ì¶”ê°€
            for config_cand in config_candidates:
                if len(valid_candidates) >= 3:
                    break
                # ì´ë¯¸ ìˆëŠ” titleê³¼ ê²¹ì¹˜ì§€ ì•Šë„ë¡
                if not any(vc['title'] == config_cand['title'] for vc in valid_candidates):
                    # ì„¤ì • ê¸°ë°˜ í›„ë³´ì— related_files ì¶”ê°€
                    if 'related_files' not in config_cand:
                        config_cand['related_files'] = _select_relevant_files_for_candidate(files_analysis, 'general')
                    valid_candidates.append(config_cand)

        except Exception as e:
            logger.error(f"ì„¤ì • ê¸°ë°˜ í›„ë³´ ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}")

    # ê·¸ë˜ë„ 3ê°œê°€ ì•ˆë˜ë©´ ê¸°ë³¸ ì˜¤ë¥˜ ì‘ë‹µìœ¼ë¡œ ì±„ì›€
    while len(valid_candidates) < 3:
        fallback_files = _select_relevant_files_for_candidate(files_analysis, 'general')
        valid_candidates.append({
            'title': 'ìš°ì„ ìˆœìœ„ ì¶”ì²œì˜¤ë¥˜',
            'priority_level': 'MEDIUM',
            'reason': 'AI ë¶„ì„ ì¤‘ ì¼ë¶€ ê²°ê³¼ê°€ ìœ íš¨í•˜ì§€ ì•Šì•„ ê¸°ë³¸ê°’ìœ¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.',
            'related_files': fallback_files
        })

    return {'priority': valid_candidates}


def _build_priority_context_from_patterns(patterns: List[Dict[str, Any]]) -> str:
    """
    ë²¡í„° ê²€ìƒ‰ ê²°ê³¼ë¥¼ LLMìš© ìš°ì„ ìˆœìœ„ ë¶„ì„ ì»¨í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
    """
    context_parts = []
    
    # ğŸš¨ ìˆ˜ì •ëœ ë¶€ë¶„: patternsê°€ ìœ íš¨í•œ ë¦¬ìŠ¤íŠ¸ì¸ì§€ í™•ì¸í•˜ëŠ” ë¡œì§ ì¶”ê°€
    if not isinstance(patterns, list) or not all(isinstance(p, dict) for p in patterns):
        logger.warning("ìœ íš¨í•˜ì§€ ì•Šì€ ìœ ì‚¬ íŒ¨í„´ ë°ì´í„°ê°€ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤. ì»¨í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return ""

    category_data = defaultdict(lambda: {'patterns': [], 'total_similarity': 0, 'priority_indicators': set(), 'complexity_levels': [], 'change_scales': []})

    for pattern in patterns:
        # ğŸš¨ ìˆ˜ì •ëœ ë¶€ë¶„: functional_category í‚¤ê°€ ìˆëŠ”ì§€ ì•ˆì „í•˜ê²Œ í™•ì¸
        category = pattern.get('functional_category')
        if not category:
            continue
            
        data = category_data[category]
        data['patterns'].append(pattern)
        data['total_similarity'] += pattern.get('similarity_score', 0)
        data['priority_indicators'].update(pattern.get('priority_indicators', set()))
        data['complexity_levels'].append(pattern.get('complexity_level'))
        data['change_scales'].append(pattern.get('change_scale'))
    
    sorted_categories = sorted(
        category_data.items(),
        key=lambda x: x[1]['total_similarity'],
        reverse=True
    )[:6]
    
    if not sorted_categories:
        return ""
    
    context_parts.append("=== ê³¼ê±° ìœ ì‚¬í•œ PRë“¤ì˜ ìš°ì„ ìˆœìœ„ íŒ¨í„´ ===")
    
    for i, (category, data) in enumerate(sorted_categories, 1):
        avg_similarity = data['total_similarity'] / len(data['patterns']) if data['patterns'] else 0
        priority_indicators = ', '.join(list(data['priority_indicators'])[:4])
        complexity_counter = Counter([c for c in data['complexity_levels'] if c])
        scale_counter = Counter([s for s in data['change_scales'] if s])
        
        most_common_complexity = complexity_counter.most_common(1)[0][0] if complexity_counter else "ë³´í†µ"
        most_common_scale = scale_counter.most_common(1)[0][0] if scale_counter else "ì†Œê·œëª¨"
        
        context_parts.append(f"""
{i}. ê¸°ëŠ¥ì˜ì—­: {category}
    - í‰ê·  ìœ ì‚¬ë„: {avg_similarity:.3f}
    - ì£¼ìš” ìš°ì„ ìˆœìœ„ ì§€í‘œ: {priority_indicators}
    - ì¼ë°˜ì  ë³µì¡ë„: {most_common_complexity}
    - ì¼ë°˜ì  ë³€ê²½ê·œëª¨: {most_common_scale}
    - ê´€ë ¨ PR ìˆ˜: {len(data['patterns'])}
        """.strip())
    
    return "\n".join(context_parts)


async def _generate_priority_candidates_with_llm(
    context: str, 
    pr_data: PreparationResult, 
    code_context: str, 
    files_analysis: Dict[str, Any],
    valid_files: List[Any]
) -> Dict[str, Any]:
    """
    LLMì„ í™œìš©í•´ ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ìœ¼ë¡œ 3ê°œì˜ ìš°ì„ ìˆœìœ„ í›„ë³´ ìƒì„± (íŒŒì¼ ì •ë³´ í¬í•¨)
    
    Args:
        context: ë²¡í„° ê²€ìƒ‰ìœ¼ë¡œ êµ¬ì„±ëœ ì»¨í…ìŠ¤íŠ¸
        pr_data: í˜„ì¬ PR ë°ì´í„°
        code_context: íŒŒì¼ë³„ ì½”ë“œ ë³€ê²½ì‚¬í•­ ì»¨í…ìŠ¤íŠ¸
        files_analysis: íŒŒì¼ ë¶„ì„ ê²°ê³¼
        valid_files: ë¶„ì„ ëŒ€ìƒ íŒŒì¼ ëª©ë¡ (ì˜ë¯¸ ì—†ëŠ” íŒŒì¼ ì œì™¸)
        
    Returns:
        Dict[str, Any]: 3ê°œì˜ ìš°ì„ ìˆœìœ„ í›„ë³´ ê²°ê³¼ (ê´€ë ¨ íŒŒì¼ í¬í•¨)
    """
    # ğŸš¨ ìˆ˜ì •ëœ ë¶€ë¶„: ìœ íš¨í•œ íŒŒì¼ ëª©ë¡ì„ ê¸°ë°˜ìœ¼ë¡œ PR ì •ë³´ ìš”ì•½
    commits_summary = ' | '.join([c.message for c in pr_data.commits[:5]])
    total_changes = sum(f.additions + f.deletions for f in valid_files)
    
    # íŒŒì¼ ë¶„ì„ ìš”ì•½
    files_summary = f"""
ê³ ìœ„í—˜ íŒŒì¼: {len(files_analysis['high_risk_files'])}ê°œ
ì¤‘ìœ„í—˜ íŒŒì¼: {len(files_analysis['medium_risk_files'])}ê°œ 
ì €ìœ„í—˜ íŒŒì¼: {len(files_analysis['low_risk_files'])}ê°œ
ë³´ì•ˆê´€ë ¨: {', '.join(files_analysis['security_related'][:3])}
DBê´€ë ¨: {', '.join(files_analysis['database_related'][:3])}
APIê´€ë ¨: {', '.join(files_analysis['api_related'][:3])}
    """.strip()
    
    system_prompt = """ë‹¹ì‹ ì€ Pull Requestì˜ ë³€ê²½ ì‚¬í•­ì„ ë¶„ì„í•˜ì—¬ ë¦¬ë·°ì–´ê°€ ë†“ì¹˜ì§€ ë§ì•„ì•¼ í•  í•µì‹¬ ë¦¬ë·° í¬ì¸íŠ¸ë¥¼ 3ê°€ì§€ ì œì•ˆí•˜ëŠ” AI ë¶„ì„ ì‹œìŠ¤í…œì…ë‹ˆë‹¤.

**ë¶„ì„ ëª©í‘œ:**
ì½”ë“œ ë³€ê²½ì˜ ì ì¬ì  ìœ„í—˜, ì¤‘ìš”í•œ ë¡œì§ ìˆ˜ì •, ì•„í‚¤í…ì²˜ ì˜í–¥ë„ë¥¼ ì‹ë³„í•˜ì—¬ ë¦¬ë·°ì–´ê°€ íš¨ìœ¨ì ìœ¼ë¡œ ë¦¬ë·°í•  ìˆ˜ ìˆë„ë¡ ë•ìŠµë‹ˆë‹¤. ê° í›„ë³´ì— ëŒ€í•´ ê´€ë ¨ëœ íŒŒì¼ íŒ¨ìŠ¤ë“¤ë„ í•¨ê»˜ ì œê³µí•©ë‹ˆë‹¤.

**ë¶„ì„ ê°€ì´ë“œë¼ì¸:**
1.  **ë³´ì•ˆ ë° ì¸ì¦ (CRITICAL)**: `jwt`, `token`, `auth`, `password` ë“± ë³´ì•ˆ ê´€ë ¨ í‚¤ì›Œë“œê°€ í¬í•¨ëœ íŒŒì¼ ë³€ê²½ì€ ìµœìš°ì„ ìœ¼ë¡œ ë¶„ì„í•©ë‹ˆë‹¤. ê¶Œí•œ ë¶€ì—¬, í† í° ìƒì„±/ê²€ì¦ ë¡œì§ì˜ ìœ„í—˜ì„±ì„ êµ¬ì²´ì ìœ¼ë¡œ ì œì‹œí•˜ì„¸ìš”.
2.  **ë°ì´í„°ë² ì´ìŠ¤ ë° API (HIGH)**: `Entity`, `Repository`, `Controller`, `DTO` ë“±ì˜ ë³€ê²½ì€ ë°ì´í„° ë¬´ê²°ì„± ë° í•˜ìœ„ í˜¸í™˜ì„± ë¬¸ì œë¥¼ ì•¼ê¸°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ìŠ¤í‚¤ë§ˆ ë³€ê²½, API ëª…ì„¸ ë³€ê²½ì˜ ì˜í–¥ì„ ëª…í™•íˆ ì„¤ëª…í•˜ì„¸ìš”.
3.  **í•µì‹¬ ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ (HIGH/MEDIUM)**: ì£¼ìš” ê¸°ëŠ¥ì˜ ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ ë³€ê²½ì„ ì‹ë³„í•©ë‹ˆë‹¤. 'ì–´ë–¤ ë¡œì§ì´ ì–´ë–»ê²Œ ë°”ë€Œì—ˆëŠ”ì§€'ì™€ 'ì–´ë–¤ ì—£ì§€ ì¼€ì´ìŠ¤ë¥¼ í…ŒìŠ¤íŠ¸í•´ì•¼ í•˜ëŠ”ì§€'ë¥¼ í¬í•¨í•˜ì—¬ ì´ìœ ë¥¼ ì‘ì„±í•˜ì„¸ìš”.
4.  **ëŒ€ê·œëª¨ ë³€ê²½ ë° ë¦¬íŒ©í† ë§ (MEDIUM)**: ì—¬ëŸ¬ íŒŒì¼ì— ê±¸ì¹œ ê´‘ë²”ìœ„í•œ ë³€ê²½ì´ë‚˜ ë¦¬íŒ©í† ë§ì€ ì•„í‚¤í…ì²˜ì˜ ì¼ê´€ì„±ì„ í•´ì¹  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë³€ê²½ì˜ ì¼ê´€ì„±ê³¼ ì ì¬ì ì¸ ì‚¬ì´ë“œ ì´í™íŠ¸ë¥¼ ê²€í† í•˜ë„ë¡ ì œì•ˆí•˜ì„¸ìš”.
5.  **ë‹¨ìˆœ ìˆ˜ì • ë° í…ŒìŠ¤íŠ¸ (LOW)**: ì˜¤íƒ€ ìˆ˜ì •, ë‹¨ìˆœ ë²„ê·¸ í”½ìŠ¤, í…ŒìŠ¤íŠ¸ ì½”ë“œ ì¶”ê°€ ë“±ì€ ë‚®ì€ ìš°ì„ ìˆœìœ„ë¥¼ ë¶€ì—¬í•©ë‹ˆë‹¤.

**`reason` ì‘ì„± ê·œì¹™:**
-   **êµ¬ì²´ì„±**: "ë¡œì§ ìˆ˜ì •"ê³¼ ê°™ì€ ì¶”ìƒì ì¸ í‘œí˜„ ëŒ€ì‹ , "A íŒŒì¼ì—ì„œ ì‚¬ìš©ì ë“±ê¸‰ ê³„ì‚° ë°©ì‹ì´ Bë¡œ ë³€ê²½ë˜ì—ˆìŠµë‹ˆë‹¤. C ì¡°ê±´ì—ì„œì˜ ì—£ì§€ ì¼€ì´ìŠ¤ë¥¼ í™•ì¸í•´ì•¼ í•©ë‹ˆë‹¤."ì™€ ê°™ì´ êµ¬ì²´ì ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”.
-   **ì‹¤í–‰ ê°€ëŠ¥ì„±**: ë¦¬ë·°ì–´ê°€ ë¬´ì—‡ì„ í™•ì¸í•´ì•¼ í•˜ëŠ”ì§€ ëª…í™•í•œ ì²´í¬í¬ì¸íŠ¸ë¥¼ ì œì‹œí•˜ì„¸ìš”.
-   **íŒŒì¼ ì°¸ì¡°**: ì–´ë–¤ íŒŒì¼ì˜ ë³€ê²½ì‚¬í•­ì— ê·¼ê±°í•œ ë¶„ì„ì¸ì§€ ëª…ì‹œí•˜ì„¸ìš”.

**`related_files` ì‘ì„± ê·œì¹™:**
-   ê° ìš°ì„ ìˆœìœ„ í›„ë³´ì™€ ì§ì ‘ì ìœ¼ë¡œ ê´€ë ¨ëœ íŒŒì¼ íŒ¨ìŠ¤ë“¤ì„ ë°°ì—´ë¡œ ì œê³µí•©ë‹ˆë‹¤.
-   ìµœëŒ€ 3ê°œê¹Œì§€ì˜ íŒŒì¼ íŒ¨ìŠ¤ë¥¼ í¬í•¨í•©ë‹ˆë‹¤.
-   ê°€ì¥ ì¤‘ìš”í•œ íŒŒì¼ë¶€í„° ìˆœì„œëŒ€ë¡œ ë‚˜ì—´í•©ë‹ˆë‹¤.
-   ê´€ë ¨ì„±ì´ ë‚®ì€ íŒŒì¼ì€ ì œì™¸í•©ë‹ˆë‹¤.

**ì‘ë‹µ í˜•ì‹:**
-   **ë°˜ë“œì‹œ 3ê°œì˜ í›„ë³´ë¥¼ ìƒì„±í•´ì•¼ í•©ë‹ˆë‹¤.**
-   **ì„¤ëª… ì—†ì´ ì•„ë˜ JSON í˜•ì‹ë§Œ ë°˜í™˜í•´ì•¼ í•©ë‹ˆë‹¤.**

{
  "priority": [
    {
      "title": "ë¶„ì„ ê²°ê³¼ì— ëŒ€í•œ êµ¬ì²´ì ì¸ ì œëª©",
      "priority_level": "CRITICAL|HIGH|MEDIUM|LOW",
      "reason": "ë¶„ì„ ê°€ì´ë“œë¼ì¸ê³¼ ì‘ì„± ê·œì¹™ì— ë”°ë¥¸ êµ¬ì²´ì ì¸ ë¦¬ë·° ìš”ì²­ ì‚¬í•­ì…ë‹ˆë‹¤.",
      "related_files": ["path/to/file1.java", "path/to/file2.java", "path/to/file3.java"]
    },
    {
      "title": "ë‘ ë²ˆì§¸ ë¶„ì„ ê²°ê³¼ ì œëª©",
      "priority_level": "CRITICAL|HIGH|MEDIUM|LOW",
      "reason": "ë‘ ë²ˆì§¸ ë¦¬ë·° ìš”ì²­ ì‚¬í•­ì…ë‹ˆë‹¤.",
      "related_files": ["path/to/file4.java", "path/to/file5.java"]
    },
    {
      "title": "ì„¸ ë²ˆì§¸ ë¶„ì„ ê²°ê³¼ ì œëª©",
      "priority_level": "CRITICAL|HIGH|MEDIUM|LOW",
      "reason": "ì„¸ ë²ˆì§¸ ë¦¬ë·° ìš”ì²­ ì‚¬í•­ì…ë‹ˆë‹¤.",
      "related_files": ["path/to/file6.java", "path/to/file7.java", "path/to/file8.java"]
    }
  ]
}
"""

    user_prompt = f"""{context}

=== í˜„ì¬ PR ì •ë³´ ===
- ì œëª©: {pr_data.title}
- ì»¤ë°‹ ë©”ì‹œì§€: {commits_summary}
- ë¸Œëœì¹˜: {pr_data.source} -> {pr_data.target}
- ì´ ë³€ê²½ë¼ì¸: {total_changes}ë¼ì¸
- íŒŒì¼ ìˆ˜: {len(valid_files)}ê°œ

=== íŒŒì¼ ë¶„ì„ ê²°ê³¼ ===
{files_summary}

=== ìƒì„¸ íŒŒì¼ ë³€ê²½ì‚¬í•­ ===
{code_context}

ìœ„ ì •ë³´ë¥¼ ì¢…í•©í•˜ì—¬ ì´ PRì˜ ë¦¬ë·° ìš°ì„ ìˆœìœ„ë¥¼ 3ê°€ì§€ ê´€ì ì—ì„œ ë¶„ì„í•´ì£¼ì„¸ìš”. ê° í›„ë³´ë§ˆë‹¤ ê´€ë ¨ëœ íŒŒì¼ íŒ¨ìŠ¤ë“¤ì„ í•¨ê»˜ 5ê°œ ì •ë„ ì œê³µí•´ì£¼ì„¸ìš”."""

    try:
        response = model.invoke([
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ])
        
        # ğŸš¨ ìˆ˜ì •ëœ ë¶€ë¶„: valid_filesë¥¼ í•¨ê»˜ ì „ë‹¬í•˜ì—¬ íŒŒì‹± ë° ê²€ì¦
        result = _parse_priority_candidates_response(response.content, valid_files, files_analysis)
        logger.info(f"LLMì´ {len(result.get('priority', []))}ê°œì˜ ìš°ì„ ìˆœìœ„ í›„ë³´ë¥¼ ìƒì„±í–ˆìŠµë‹ˆë‹¤")
        return result
        
    except Exception as e:
        logger.error(f"LLM ìš°ì„ ìˆœìœ„ í›„ë³´ ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}")
        # í´ë°±: ì„¤ì • ê¸°ë°˜ ìš°ì„ ìˆœìœ„ í›„ë³´ ê³„ì‚°
        return calculate_config_based_priority_candidates(pr_data)


def _parse_priority_candidates_response(
    response_text: str, 
    valid_files: List[Any], 
    files_analysis: Dict[str, Any]
) -> Dict[str, Any]:
    """
    LLM ìš°ì„ ìˆœìœ„ í›„ë³´ ì‘ë‹µì„ íŒŒì‹± (íŒŒì¼ ì •ë³´ í¬í•¨)
    
    Args:
        response_text: LLM ì‘ë‹µ í…ìŠ¤íŠ¸
        valid_files: í•„í„°ë§ëœ PR ë°ì´í„° íŒŒì¼ ëª©ë¡
        files_analysis: íŒŒì¼ ë¶„ì„ ê²°ê³¼
        
    Returns:
        Dict[str, Any]: íŒŒì‹±ëœ 3ê°œì˜ ìš°ì„ ìˆœìœ„ í›„ë³´ ê²°ê³¼ (ê´€ë ¨ íŒŒì¼ í¬í•¨)
    """
    try:
        import json
        import re
        
        # JSON ë¶€ë¶„ë§Œ ì¶”ì¶œ
        json_match = re.search(r'\{.*\}', response_text, re.DOTALL)
        if not json_match:
            raise ValueError("JSON í˜•ì‹ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        json_str = json_match.group(0)
        result = json.loads(json_str)
        
        # ê²€ì¦ ë° ì •ë¦¬
        candidates = result.get('priority', [])
        valid_candidates = []
        
        for i, candidate in enumerate(candidates[:3]):  # ìµœëŒ€ 3ê°œê¹Œì§€
            if not isinstance(candidate, dict):
                continue
                
            title = candidate.get('title', f'ìš°ì„ ìˆœìœ„ í›„ë³´ {i+1}')
            priority_level = candidate.get('priority_level', 'MEDIUM')
            reason = candidate.get('reason', 'ë¶„ì„ ê²°ê³¼ì— ë”°ë¥¸ ìš°ì„ ìˆœìœ„')
            related_files = candidate.get('related_files', [])
            
            # ìš°ì„ ìˆœìœ„ ë ˆë²¨ ê²€ì¦
            if priority_level not in ['CRITICAL', 'HIGH', 'MEDIUM', 'LOW']:
                priority_level = 'MEDIUM'
            
            # related_files ê²€ì¦ ë° ë³´ì™„
            if not isinstance(related_files, list):
                related_files = []
            
            # related_filesê°€ ë¹„ì–´ìˆìœ¼ë©´ íŒŒì¼ ë¶„ì„ ê²°ê³¼ë¡œ ì¶”ì •
            if not related_files:
                candidate_content = (title + ' ' + reason).lower()
                if any(keyword in candidate_content for keyword in ['security', 'auth', 'token', 'password']):
                    related_files = _select_relevant_files_for_candidate(files_analysis, 'security')
                elif any(keyword in candidate_content for keyword in ['database', 'entity', 'repository']):
                    related_files = _select_relevant_files_for_candidate(files_analysis, 'database')
                elif any(keyword in candidate_content for keyword in ['api', 'controller', 'endpoint']):
                    related_files = _select_relevant_files_for_candidate(files_analysis, 'api')
                else:
                    related_files = _select_relevant_files_for_candidate(files_analysis, 'general')
            
            # íŒŒì¼ íŒ¨ìŠ¤ ê²€ì¦ ë° ì •ë¦¬
            final_files = []
            # ğŸš¨ ìˆ˜ì •ëœ ë¶€ë¶„: í•„í„°ë§ëœ valid_filesë¥¼ ì‚¬ìš©í•˜ì—¬ íŒŒì¼ëª… ëª©ë¡ì„ ê°€ì ¸ì˜´
            all_filenames = [f.filename for f in valid_files]
            
            for file_path in related_files[:5]:  # ìµœëŒ€ 5ê°œ
                if isinstance(file_path, str):
                    # ì‹¤ì œ PRì— í¬í•¨ëœ íŒŒì¼ì¸ì§€ í™•ì¸
                    if file_path in all_filenames:
                        final_files.append(file_path)
                    else:
                        # ë¶€ë¶„ ë§¤ì¹­ìœ¼ë¡œ ì°¾ê¸°
                        matching_files = [f for f in all_filenames if file_path.split('/')[-1] in f]
                        if matching_files:
                            final_files.append(matching_files[0])
            
            # ì—¬ì „íˆ related_filesê°€ ë¶€ì¡±í•˜ë©´ ë³€ê²½ì‚¬í•­ì´ í° íŒŒì¼ë“¤ë¡œ ë³´ì™„
            if len(final_files) < 3:
                sorted_files = sorted(valid_files, key=lambda f: f.additions + f.deletions, reverse=True)
                for file in sorted_files:
                    if file.filename not in final_files and len(final_files) < 5:
                        final_files.append(file.filename)
            
            valid_candidates.append({
                'title': title,
                'priority_level': priority_level,
                'reason': reason,
                'related_files': final_files
            })
        
        # 3ê°œ ë¯¸ë§Œì¸ ê²½ìš° ì„¤ì • ê¸°ë°˜ í›„ë³´ë¡œ ì±„ì›€
        while len(valid_candidates) < 3:
            try:
                config_candidates = calculate_config_based_priority_candidates(pr_data)
                missing_candidate = config_candidates['candidates'][len(valid_candidates)]
                
                # ì„¤ì • ê¸°ë°˜ í›„ë³´ì— related_files ì¶”ê°€
                if 'related_files' not in missing_candidate:
                    missing_candidate['related_files'] = _select_relevant_files_for_candidate(files_analysis, 'general')
                
                valid_candidates.append(missing_candidate)
            except:
                # ê¸°ë³¸ í›„ë³´ ì¶”ê°€
                fallback_files = _select_relevant_files_for_candidate(files_analysis, 'general')
                valid_candidates.append({
                    'title': f'ìš°ì„ ìˆœìœ„ í›„ë³´ {len(valid_candidates) + 1}',
                    'priority_level': 'MEDIUM',
                    'reason': 'íŒŒì¼ ë³€ê²½ì‚¬í•­ ê¸°ë°˜ ë¶„ì„ ê²°ê³¼',
                    'related_files': fallback_files
                })
                break
        
        return {
            'priority': valid_candidates,
        }
        
    except Exception as e:
        logger.error(f"ìš°ì„ ìˆœìœ„ í›„ë³´ ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨: {str(e)}")
        # í´ë°±: ì„¤ì • ê¸°ë°˜ í›„ë³´ì— íŒŒì¼ ì •ë³´ ì¶”ê°€
        try:
            fallback_result = calculate_config_based_priority_candidates(pr_data)
            for candidate in fallback_result['candidates']:
                if 'related_files' not in candidate:
                    candidate['related_files'] = _select_relevant_files_for_candidate(files_analysis, 'general')
            return fallback_result
        except:
            return _get_default_error_response_with_files(valid_files)

# ì´ í•¨ìˆ˜ëŠ” ìˆ˜ì •ì´ í•„ìš”í•˜ì§€ ì•Šì§€ë§Œ, ì™„ì „í•œ ì½”ë“œë¥¼ ìœ„í•´ í¬í•¨í•©ë‹ˆë‹¤.
def _validate_and_complete_candidates(result: Dict[str, Any], pr_data: PreparationResult, files_analysis: Dict[str, Any]) -> Dict[str, Any]:
    """ê²°ê³¼ë¥¼ ê²€ì¦í•˜ê³ , 3ê°œê°€ ì•ˆë˜ë©´ ì„¤ì • ê¸°ë°˜ ë˜ëŠ” ê¸°ë³¸ê°’ìœ¼ë¡œ ì±„ì›€ (íŒŒì¼ ì •ë³´ í¬í•¨)"""
    
    candidates = result.get('priority', [])
    valid_candidates = []

    for i, candidate in enumerate(candidates[:3]):
        if not isinstance(candidate, dict) or not all(k in candidate for k in ['title', 'priority_level', 'reason']):
            continue
        
        priority_level = candidate.get('priority_level', 'MEDIUM')
        if priority_level not in ['CRITICAL', 'HIGH', 'MEDIUM', 'LOW']:
            priority_level = 'MEDIUM'
        
        # related_filesê°€ ì—†ìœ¼ë©´ íŒŒì¼ ë¶„ì„ ê²°ê³¼ë¡œ ì¶”ê°€
        related_files = candidate.get('related_files', [])
        if not related_files:
            # í›„ë³´ì˜ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ê´€ë ¨ íŒŒì¼ ì¶”ì •
            candidate_content = (candidate.get('title', '') + ' ' + candidate.get('reason', '')).lower()
            if 'security' in candidate_content or 'auth' in candidate_content:
                related_files = _select_relevant_files_for_candidate(files_analysis, 'security')
            elif 'database' in candidate_content or 'entity' in candidate_content:
                related_files = _select_relevant_files_for_candidate(files_analysis, 'database')
            elif 'api' in candidate_content or 'controller' in candidate_content:
                related_files = _select_relevant_files_for_candidate(files_analysis, 'api')
            else:
                related_files = _select_relevant_files_for_candidate(files_analysis, 'general')
        
        valid_candidates.append({
            'title': candidate.get('title', f'ìš°ì„ ìˆœìœ„ í›„ë³´ {i+1}'),
            'priority_level': priority_level,
            'reason': candidate.get('reason', 'ë¶„ì„ ê²°ê³¼ì— ë”°ë¥¸ ìš°ì„ ìˆœìœ„'),
            'related_files': related_files[:5]  # ìµœëŒ€ 5ê°œë¡œ ì œí•œ
        })

    # 3ê°œ ë¯¸ë§Œì¸ ê²½ìš°, ë¨¼ì € ì„¤ì • ê¸°ë°˜ í›„ë³´ë¡œ ì±„ìš°ê³ , ê·¸ë˜ë„ ë¶€ì¡±í•˜ë©´ ê¸°ë³¸ ì˜¤ë¥˜ê°’ìœ¼ë¡œ ì±„ì›€
    if len(valid_candidates) < 3:
        try:
            config_candidates = calculate_config_based_priority_candidates(pr_data)['candidates']
            
            # ì„¤ì • ê¸°ë°˜ í›„ë³´ì—ì„œ ì¤‘ë³µë˜ì§€ ì•ŠëŠ” ê²ƒì„ ì¶”ê°€
            for config_cand in config_candidates:
                if len(valid_candidates) >= 3:
                    break
                # ì´ë¯¸ ìˆëŠ” titleê³¼ ê²¹ì¹˜ì§€ ì•Šë„ë¡
                if not any(vc['title'] == config_cand['title'] for vc in valid_candidates):
                    # ì„¤ì • ê¸°ë°˜ í›„ë³´ì— related_files ì¶”ê°€
                    if 'related_files' not in config_cand:
                        config_cand['related_files'] = _select_relevant_files_for_candidate(files_analysis, 'general')
                    valid_candidates.append(config_cand)

        except Exception as e:
            logger.error(f"ì„¤ì • ê¸°ë°˜ í›„ë³´ ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}")

    # ê·¸ë˜ë„ 3ê°œê°€ ì•ˆë˜ë©´ ê¸°ë³¸ ì˜¤ë¥˜ ì‘ë‹µìœ¼ë¡œ ì±„ì›€
    while len(valid_candidates) < 3:
        fallback_files = _select_relevant_files_for_candidate(files_analysis, 'general')
        valid_candidates.append({
            'title': 'ìš°ì„ ìˆœìœ„ ì¶”ì²œì˜¤ë¥˜',
            'priority_level': 'MEDIUM',
            'reason': 'AI ë¶„ì„ ì¤‘ ì¼ë¶€ ê²°ê³¼ê°€ ìœ íš¨í•˜ì§€ ì•Šì•„ ê¸°ë³¸ê°’ìœ¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.',
            'related_files': fallback_files
        })

    return {'priority': valid_candidates}

